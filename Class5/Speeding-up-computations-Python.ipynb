{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Speeding-Up Computations\n",
    "**Please Note**: Approaches presented in each section are listed in no particular order -- some (or many) may work better for your use case; performance of approach may differ on data set, where it's hosted, computing resources, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import inspect\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Timing results](https://stackoverflow.com/questions/17579357/time-time-vs-timeit-timeit): `%%time` and `%%timeit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-up Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"https://s3.amazonaws.com/h2o-airlines-unpacked/year2012.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 4.76 s, total: 20.2 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(filepath_or_buffer=file_name,\n",
    "                 encoding='latin-1'\n",
    "                )\n",
    "# df = pd.read_csv(\"../Class3/2012.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing output, per [SO](https://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1/556411#556411):\n",
    "- **Wall time**: \"time from start to finish of the call\". \n",
    "- **User time**: CPU time outside the kernel within the process, such as in library code. \n",
    "- **Sys time**: CPU time inside the kernel within the process. \n",
    "- **User + Sys time**: how much actual CPU time your process used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6096762, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 496 ms, sys: 25.4 ms, total: 521 ms\n",
      "Wall time: 517 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MQ     473140\n",
       "DL     726879\n",
       "WN    1140535\n",
       "AA     525220\n",
       "EV     740855\n",
       "FL     218162\n",
       "US     404263\n",
       "UA     531245\n",
       "VX      54742\n",
       "YV     133976\n",
       "B6     229056\n",
       "OO     617756\n",
       "AS     147569\n",
       "HA      74109\n",
       "F9      79255\n",
       "Name: UniqueCarrier, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df[\"UniqueCarrier\"].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Recode missing values to large negative # as compensated delays are large positive #s:\n",
    "df[\"DepDelay\"] = df[\"DepDelay\"].fillna(-9999)\n",
    "df[\"ArrDelay\"] = df[\"ArrDelay\"].fillna(-9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. `pandas` - Read File in Chunks\n",
    "[Reference](https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 6.22 s, total: 31.4 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create an object for iteration over, to spead-up read-ing process:\n",
    "df_chunked = pd.read_csv(filepath_or_buffer=file_name,\n",
    "                         encoding='latin-1',\n",
    "                         chunksize=1000000)\n",
    "\n",
    "# Create a list to store data set chunks:\n",
    "chunk_list = []\n",
    "\n",
    "\n",
    "# Each chunk is in df format\n",
    "for chunk in df_chunked:  \n",
    "    chunk_list.append(chunk)\n",
    "else:\n",
    "    # concat the list into dataframe \n",
    "    df = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. (If Possible) Perform in-database Computations\n",
    "- **Approach** (if possible), as we did in [Class 2](https://goo.gl/JkLxHq):\n",
    "  1. Connect script to database\n",
    "  2. Perform aggregations and variable transformations in-database\n",
    "  3. Send results back to script\n",
    "- **Please note**, this may not be possible, because:\n",
    "  - data is not in database to begin with, or\n",
    "  - time to put data into database to aggegate in, is too time consuming, or\n",
    "  - you're querying a production database -- and running this query may slow-down performance of services running in production that depend on this database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. `dask` -- Read File as is\n",
    "[Documentation](http://docs.dask.org/en/latest/) and more [examples](https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s ± 23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dd.read_csv(file_name, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 27.9 ms, total: 134 ms\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_dask = dd.read_csv(file_name,\n",
    "                      encoding='latin-1',\n",
    "                      assume_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. `pySpark` -- Read File as is\n",
    "[Documentation](https://spark.apache.org/docs/2.2.0/) and more examples of Airlines data set [analysis](https://github.com/goldshtn/spark-workshop/blob/master/scala/lab2-airlines.md) in `pySpark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go this [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2873656090782679/2252219718088020/7574928746777728/latest.html) to view `pySpark` code that reads-in Airlines dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time duration (seconds) = 90 seconds (0.02 to get `file_name`, 60.02 to read into DBFS and 29.93 to read into notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code, you'll need a Databricks account (see class slides on instructions) to `Import Notebook` into. (`Import Notebook` prompt is at top-right of screen of the [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2873656090782679/2252219718088020/7574928746777728/latest.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for reading-in Airlines data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-Up Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `pandas`\n",
    "Suggested (non-exhaustive) list of approaches:\n",
    "- Pre-allocate (vs `append()` to) lists, etc.\n",
    "- Drop [unnecessary columns](https://realpython.com/python-data-cleaning-numpy-pandas/#dropping-columns-in-a-dataframe)\n",
    "- Create a better index for [faster subsetting](https://realpython.com/python-data-cleaning-numpy-pandas/#changing-the-index-of-a-dataframe)\n",
    "- Type optimization of variables in dataset, per [this](https://www.dataquest.io/blog/pandas-big-data/) and [this](https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e) blog post\n",
    "- Saving intermediate results in HDF5 store, per Wes McKinney's book [Python for Data Analysis](http://wesmckinney.com/pages/book.html) and this [blog post](https://realpython.com/fast-flexible-pandas/#prevent-reprocessing-with-hdfstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a. `for-loop` versus `apply()` versus `applymap()` versus `cut()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal** [as in Class 2](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class2/Intro-to-pandas.ipynb): Convert delays from minutes to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.shape[0]\n",
    "dep_delay_hr = [None] * num_rows\n",
    "col_index = np.where(df.columns == 'DepDelay')[0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.9 s, sys: 232 ms, total: 48.1 s\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(num_rows):\n",
    "    dep_delay_hr[i] = df.iloc[i, col_index]/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 209 ms, total: 1.36 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dep_delay_hr_apply = df['DepDelay'].apply(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.3 ms ± 3.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "delay_hr_apply = df[['DepDelay', 'ArrDelay']].apply(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.88 s ± 30 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "delay_hr_applymap = df[['DepDelay', 'ArrDelay']].applymap(lambda x: x/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for converting minutes to hours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal** [as in Class 2](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class2/Intro-to-pandas.ipynb): Convert continuous variable into categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_departure_delays(delay_min):\n",
    "    if (delay_min >= -60.0) & (delay_min < 15.0):\n",
    "        return \"no_delay\"\n",
    "    elif (delay_min >= 15.0) & (delay_min < 30.0):\n",
    "        return \"small_delay\"\n",
    "    elif (delay_min >= 30.0) & (delay_min < 60.0):\n",
    "        return \"medium_delay\"        \n",
    "    elif (delay_min >= 60.0) & (delay_min < 120.0):\n",
    "        return \"big_delay\"        \n",
    "    elif (delay_min >= 120.0):\n",
    "        return \"compensated_delay\"        \n",
    "    else:\n",
    "        return \"missing_delay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 78.2 ms, total: 1.94 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "delay_bin = df['DepDelay'].apply(lambda x: bin_departure_delays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_delay             5027921\n",
       "small_delay           388931\n",
       "medium_delay          303344\n",
       "big_delay             199614\n",
       "compensated_delay     101221\n",
       "missing_delay          75731\n",
       "Name: DepDelay, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 692 ms, sys: 38.5 ms, total: 731 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Redo recoding of missing values, per \n",
    "# https://stackoverflow.com/questions/54922775/using-magic-command-timeit-n1-r1-causes-jupyter-does-not-keep-the-value-of\n",
    "delay_bin_cut = pd.cut(df['DepDelay'].fillna(-9999),\n",
    "                       bins=[-20000, -60.0, 15, 30, 60, 120, 10000],\n",
    "                       labels=[\"missing_delay\", \"no_delay\", \"small_delay\", \"medium_delay\", \"big_delay\", \"compensated_delay\"],\n",
    "                       include_lowest=True,\n",
    "                       right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_delay             5027921\n",
       "small_delay           388931\n",
       "medium_delay          303344\n",
       "big_delay             199614\n",
       "compensated_delay     101221\n",
       "missing_delay          75731\n",
       "Name: DepDelay, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_bin_cut.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More [examples](https://realpython.com/fast-flexible-pandas/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for binning departure delays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### b. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual explanation of [vectorization:](https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/)\n",
    "\n",
    "![Visual explanation of what vectorization is](./images/vectorization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal** [As in Class 3](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class3/Intro-to-sklearn.ipynb): Create outcome variable for compensated delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delays_requiring_compensation(arrival_delay, departure_delay):\n",
    "    \"\"\"Fcn to return if arrival and/or departure delay resulted in passenger\n",
    "       compensation.\n",
    "       \n",
    "       Arguments:\n",
    "           - arrival_delay:   delay in minutes\n",
    "           - departure_delay: delay in minutes\n",
    "       \n",
    "       Returns:\n",
    "           - number of delays (arrival and or departure) that were delayed\n",
    "             so long that passenger got compensated\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    if (arrival_delay >= 3.0 * 60.0) | (departure_delay >= 2.0 * 60.0):\n",
    "        # If arrival delay is 3+ hours, or if departure delay is 2+ hours:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 335 ms, total: 2min 7s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['compensated_delays'] = df[['ArrDelay', 'DepDelay']].apply(\n",
    "    lambda row: delays_requiring_compensation(row[0], row[1]),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5995130, 1: 101632})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prerequisite for vectorizing with Boolean logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(True | True)\n",
    "print(True | False)\n",
    "print(False | True)\n",
    "print(False | False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def delays_requiring_compensation_vec(arrival_delay, departure_delay):\n",
    "    \"\"\"Fcn to return if arrival and/or departure delay resulted in passenger\n",
    "       compensation.\n",
    "       \n",
    "       Arguments:\n",
    "           - arrival_delay:   delay in minutes\n",
    "           - departure_delay: delay in minutes\n",
    "       \n",
    "       Returns:\n",
    "           - number of delays (arrival and or departure) that were delayed\n",
    "             so long that passenger got compensated\n",
    "    \"\"\"\n",
    "    count_arrival_delays = arrival_delay >= (3 * 60.0)\n",
    "    count_departure_delays = departure_delay >= (2 * 60.0)\n",
    "    # Leveraging Boolean logic:\n",
    "    compensated_delays = count_arrival_delays | count_departure_delays\n",
    "    return compensated_delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 ms, sys: 8.35 ms, total: 50.6 ms\n",
      "Wall time: 20.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['compensated_delays_vec'] = delays_requiring_compensation_vec(df['ArrDelay'], \n",
    "                                                                 df['DepDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 5995130, True: 101632})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays_vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More [examples](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### c. `numpy` Operations via `.values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ArrDelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ArrDelay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 23.5 ms, total: 328 ms\n",
      "Wall time: 86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Redo recoding of missing values, per \n",
    "# https://stackoverflow.com/questions/54922775/using-magic-command-timeit-n1-r1-causes-jupyter-does-not-keep-the-value-of\n",
    "df[\"DepDelay\"] = df[\"DepDelay\"].fillna(-9999)\n",
    "df[\"ArrDelay\"] = df[\"ArrDelay\"].fillna(-9999)\n",
    "df['compensated_delays_vec_np'] = delays_requiring_compensation_vec(df['ArrDelay'].values, \n",
    "                                                                    df['DepDelay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 5995130, True: 101632})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['compensated_delays_vec_np'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples: [here](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6) and [here](https://jakevdp.github.io/PythonDataScienceHandbook/02.04-computation-on-arrays-aggregates.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for processing multiple columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What does the difference in counts between `(a)` and ( `(b)` or `(c)` ) tell us about the nature of delays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. In-database Computations\n",
    "Please see Section \"Speeding-up Data Read\" (above) for more information and caveats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 s, sys: 11.5 s, total: 51.8 s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WN    1140535\n",
       "EV     740855\n",
       "DL     726879\n",
       "OO     617756\n",
       "UA     531245\n",
       "AA     525220\n",
       "MQ     473140\n",
       "US     404263\n",
       "B6     229056\n",
       "FL     218162\n",
       "AS     147569\n",
       "YV     133976\n",
       "F9      79255\n",
       "HA      74109\n",
       "VX      54742\n",
       "Name: UniqueCarrier, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_dask['UniqueCarrier'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Warning](./images/warning.png) Per [bug submission](https://github.com/dask/dask/issues/442), while Dask's `value_counts()` [documentation](http://docs.dask.org/en/latest/dataframe-api.html) states that you can sort results as in `pandas`, Dask does not have that functionality implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. `pySpark` + `SparkSQL`\n",
    "Please go this [Databricks notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2873656090782679/2252219718088020/7574928746777728/latest.html) to view `SparkSQL` code that performs counts by airline carrier for Airlines dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time duration (seconds) = 7.35 seconds (0.05 s to specify that we'll be running SparkSQL against dataset, 7.33 s to perform aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which was the fastest for getting number of flight paths by carrier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding-up Embarrassingly Parallel Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Class 4](https://github.com/ikukuyeva/Stats-404-W19-Statistical-Computing/blob/master/Class4/Fashion-MNIST.ipynb): We estimated 7 different Random Forest models in serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to repository on my machine:\n",
    "fashion_mnist_dir = \"/Users/irina/Documents/Stats-Related/Fashion-MNIST-repo\"\n",
    "os.chdir(fashion_mnist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST data set using helper function from Fashion-MNIST repository:\n",
    "from utils import mnist_reader\n",
    "# Load 10K images for this demo:\n",
    "X, y = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   33.0s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a. (Potentially) Leverage Parralel Backend of `sklearn` model\n",
    "Example: `sklearn` RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per sklearn documentation of [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "`n_jobs` : int or None, optional (default=None)\n",
    "\n",
    "The number of jobs to run in parallel for **both fit and predict**. None means 1 unless in a joblib.parallel_backend context. \n",
    "-1 means using all processors...\n",
    "\n",
    "Note: Parallel backend is [joblib](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=2)]: Done 500 out of 500 | elapsed:   17.6s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base_parallel2 = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=2).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much speed-up did we get by using 2 cores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:   11.4s finished\n"
     ]
    }
   ],
   "source": [
    "rf_base_parallel4 = RandomForestClassifier(n_estimators=500,\n",
    "                                 min_samples_leaf=30,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=2019,\n",
    "                                 class_weight='balanced',\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=4).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is speed-up not 4x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### b. `joblib` for Embarrassingly Parallel Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_spec(num_trees, features=X, outcome=y):\n",
    "    ### --- RF model to estimate:\n",
    "    rf = RandomForestClassifier(n_estimators=num_trees,\n",
    "                                min_samples_leaf=30,\n",
    "                                oob_score=True,\n",
    "                                random_state=2019,\n",
    "                                class_weight='balanced',\n",
    "                                verbose=1)\n",
    "    ### --- Estimate RF model and save estimated model:\n",
    "    rf.fit(features, outcome)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [50, 100, 250, 500, 1000, 1500, 2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Baseline for Estimating 7 RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   31.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2500 out of 2500 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "for num_trees in n_trees:\n",
    "    rf_spec(num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Leveraging Parallelization to Estimate Forests Simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   45.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1500 out of 1500 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 2500 out of 2500 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=4)]: Done   7 out of   7 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Per http://academic.bancey.com/parallelization-in-python-example-with-joblib/\n",
    "results = Parallel(n_jobs=4, verbose=1, backend=\"threading\")(map(delayed(rf_spec), n_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Which is fastest for parallelizing RF model estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aside: [Explanation](https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python) of `delayed` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no clear tech stack winner for which solution will speed-up computations for each use case; speed depends on:\n",
    "  - size of dataset\n",
    "  - analyses you want to perform\n",
    "  - computing architecture\n",
    "  - (many others)\n",
    " \n",
    " \n",
    "- Airlines data set (using 2012 flight paths only) might be too small for our pySpark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further Reading\n",
    "- [High Performance Python](http://shop.oreilly.com/product/0636920028963.do)\n",
    "  - Appropriate usage of lists vs tuples \n",
    "  - Iterators and Generators\n",
    "  - Compiling to C\n",
    "  - (more on) Cluster Computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-stats404",
   "language": "python",
   "name": "env-stats404"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
